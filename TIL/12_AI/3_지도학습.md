# ì§€ë„í•™ìŠµ



* ì—í¬í¬

  * ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í•œë²ˆì˜ í›ˆë ¨ íšŸìˆ˜

* ë°°ì¹˜ë€

  * ê°€ì¤‘ì¹˜ ë“±ì˜ ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ì–‘

* ë°°ì¹˜ í¬ê¸°ì— ë‹¤ë¥¸ ê²½ì‚¬ í•˜ê°•ë²•

  * ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•
    * í•œ ë²ˆì˜ ì—í¬í¬ì— ëŒ€í•œ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ ë‹¨ í•œ ë²ˆ ìˆ˜í–‰
    * ì „ì²´ ë°ì´í„°ë¥¼ ê³ ë ¤í•´ì„œ í•™ìŠµ
  * í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•
    * ëœë¤ìœ¼ë¡œ ì„ íƒí•œ í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ëŠ” ë°©ë²•
    * ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ë³´ë‹¤ëŠ” ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆì§€ë§Œ í•˜ë‚˜ì˜ ë°ì´í„°ë§Œ ì €ì¥í•˜ë©´ ë˜ë¯€ë¡œ ìì›ì´ ì ì€ ì»´í“¨í„°ì—ì„œë„ ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥
  * ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•
    * ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì •í•˜ì—¬ í•´ë‹¹ ë°ì´í„° ê°œìˆ˜ë§Œí¼ì— ëŒ€í•´ì„œ ê³„ì‚°
    * ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ ì¡°ì •

* ì˜µí‹°ë§ˆì´ì €

  * ëª¨ë©˜í…€

    * ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ê³„ì‚°ëœ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ì— í•œ ì‹œì  ì „ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸° ê°’ì„ ì¼ì •í•œ ë¹„ìœ¨ë§Œí¼ ë°˜ì˜

      ![image-20220826154542446](3_ì§€ë„í•™ìŠµ.assets/image-20220826154542446.png)

    * ê´€ì„±ì˜ í˜ì„ ë¹Œë ¤ ë¡œì»¬ ë¯¸ë‹ˆë©ˆì„ ë§Œë‚¬ì„ ë•Œ ë©ˆì¶”ì§€ ì•Šê³  ê¸€ë¡œë²Œ ë¯¸ë‹ˆë©ˆì„ ê³„ì† ì°¾ìœ¼ë ¤ê³  ë…¸ë ¥

    * `torch.optim.SGD(params, lr=required parameter, momentum=0, dampening=0, weight_decay=0, nesterov=False)`

  * ì•„ë‹¤ê·¸ë¼ë“œ(Adagrad)

    * ê° ë§¤ê°œë³€ìˆ˜ì— ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš©
    * `torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)`

  * ì•Œì— ì—ìŠ¤í”„ë¡­(RMSprop)

    * ì•„ë‹¤ê·¸ë¼ë“œì˜ ì ì°¨ í•™ìŠµë¥ ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³´ì™„
    * `torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)`

  * ì•„ë‹´(Adam)

    * RMSpropê³¼ Momentum ë‘ ê°€ì§€ í•©ì¹œ ë°©ì‹
    * `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)`



## linear regression

ë…ë¦½ë³€ìˆ˜ xì™€ ì¢…ì†ë³€ìˆ˜ yì˜ ê´€ê³„ë¥¼ ì„ í˜• ëª¨ë¸ë¡œ ëª¨ë¸ë§

-> ì„ì˜ì˜ ë…ë¦½ë³€ìˆ˜ xë¥¼ ë„£ì—ˆì„ ë•Œì˜ ì¶œë ¥ì„ ì˜ˆì¸¡

ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” íŒŒë€ìƒ‰ ì„ ì„ ì˜ˆì¸¡

![image-20220826152047510](3_ì§€ë„í•™ìŠµ.assets/image-20220826152047510.png)

### hypothesis (ê°€ì„¤ ì„¸ìš°ê¸°)

ì„ í˜•ì ì¼ ê²ƒì´ë¼ëŠ” ê°€ì •: ğ»(ğ‘¥)=ğ‘Šğ‘¥+ğ‘ `hypothesis = x_train * W + b`



### cost function

ê°€ì„¤ì˜ ê²°ê³¼ì™€ ë‹µì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•œ í•¨ìˆ˜ (MSE)

![image-20220826152428758](3_ì§€ë„í•™ìŠµ.assets/image-20220826152428758.png)

`cost = torch.mean((hypothesis - y_train) ** 2)`



### gradient decent

ìœ„ì—ì„œ êµ¬í•œ cost functionì„ ë¯¸ë¶„ 

![image-20220826152521326](3_ì§€ë„í•™ìŠµ.assets/image-20220826152521326.png)

ì´ë•Œ a(ì•ŒíŒŒ)ëŠ” Learning rateë¡œ í•œë²ˆ update í• ë•Œ, í•´ë‹¹ ê¸°ìš¸ê¸°ë¡œ ì–¼ë§Œí¼ ì—…ë°ì´íŠ¸ í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë©”í„°, ì‚¬ìš©ìê°€ ì„ì˜ë¡œ ì„¤ì •í•˜ëŠ” ê°’



### health_dataë¡œ ì‹¤ìŠµ

1. google driveì—ì„œ ê°€ì ¸ì˜¤ê¸°

   ```python
   from google.colab import drive
   drive.mount('/content/gdrive')
   
   import pathlib
   path = pathlib.Path('/content/gdrive/My Drive/health_data.csv') 
   ```

2. NaN í–‰ ì œê±°í•˜ê³ , shape ë§Œë“¤ê¸°

   ```python
   df = pd.read_csv(path)
   df = df.dropna(axis = 0).reset_index(drop=True)
   height = torch.tensor(df.height)
   weight = torch.tensor(df.weight)
   x_train = height.view([height.shape[0],1]).float() 
   y_train = weight.view([weight.shape[0],1]).float()
   df.head()
   ```

3. ì‹œê°í™”í•˜ê¸°

   ```python
   %matplotlib inline
   import matplotlib.pyplot as plt
   
   x = x_train
   y = y_train
   
   plt.scatter(x, y)
   plt.xlabel('height (cm)')
   plt.ylabel('weight (kg)')
   plt.show()
   ```

4. ë°ì´í„° ì •ê·œí™”

   ![image-20220826153244810](3_ì§€ë„í•™ìŠµ.assets/image-20220826153244810.png)

   ```python
   # data normailization
   x_min, x_max = x_train.min(), x_train.max() # xì˜ ìµœëŒ€, ìµœì†Ÿê°’
   y_min, y_max = y_train.min(), y_train.max() # yì˜ ìµœëŒ€, ìµœì†Ÿê°’
   x = (x_train-x_min)/(x_max-x_min)
   y = (y_train-y_min)/(y_max-y_min)
   
   # ê°€ì¤‘ì¹˜ì™€, í¸í–¥ ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”
   # requires_grad = True: ì´ ë³€ìˆ˜ëŠ” í•™ìŠµì„ í†µí•´ ê³„ì† ê°’ì´ ë³€ê²½ë˜ëŠ” ë³€ìˆ˜ì„
   W = torch.zeros(1, requires_grad=True)
   b = torch.zeros(1, requires_grad=True)
   
   # optimizer ì„¤ì •
   optimizer = optim.Adam([W, b], lr=0.1)
   
   # ê²½ì‚¬ í•˜ê°•ë²•ì„ ë°˜ë³µí•  íšŸìˆ˜ ì„¤ì •
   epochs = 1000
   
   for epoch in range(epochs + 1):
       hypothesis = W*x + b
       cost = torch.mean((hypothesis - y) ** 2)
       
       # costë¡œ H(x) ê°œì„ ì„ ìœ„í•œ update 
       optimizer.zero_grad()
       cost.backward()
       optimizer.step()
       
       # 1000ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
       if epoch % 1000 == 0:
           print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
               epoch, epochs, W.item(), b.item(), cost.item()
           ))
   ```

5. ë°ì´í„° ì‹œê°í™”

   ```python
   # ì§ì„  ì‹œê°í™”
   %matplotlib inline
   import matplotlib.pyplot as plt
   import numpy as np
   
   x = x_train
   y = y_train
   plt.scatter(x, y)
   
   t = np.arange(140.,190.,0.001)
   
   # të¥¼ normalizeí•œ ë’¤ yê°’ ì˜ˆì¸¡
   t_norm = (t-x_min.numpy())/(x_max.numpy()-x_min.numpy())
   output = W.item()*t_norm+b.item()
   
   # yë¥¼ denormalize
   y_denorm = (output)*(y_max.numpy()-y_min.numpy())+y_min.numpy()
   
   plt.plot(t, y_denorm)
   plt.xlabel('height (cm)')
   plt.ylabel('weight (kg)')
   plt.show()
   ```

   ![image-20220826160403536](3_ì§€ë„í•™ìŠµ.assets/image-20220826160403536.png)

   



```
costë¡œ H(x) ê°œì„ ì„ ìœ„í•œ update
1. gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
   ë¯¸ë¶„ì„ í†µí•´ ì–»ì€ ê¸°ìš¸ê¸°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” -> ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ í¸í–¥ì— ëŒ€í•´ì„œ ìƒˆë¡œìš´ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ
   optimizer.zero_grad()
   
2. ë¹„ìš©í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ì—¬ gradient ê³„ì‚°
   ê°€ì¤‘ì¹˜ Wì™€ í¸í–¥ bì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°
   cost.backward()
   
3. Wì™€ bë¥¼ ì—…ë°ì´íŠ¸
   ì¸ìˆ˜ë¡œ ë“¤ì–´ê°”ë˜ Wì™€ bì—ì„œ ë¦¬í„´ë˜ëŠ” ë³€ìˆ˜ë“¤ì˜ ê¸°ìš¸ê¸°ì— í•™ìŠµë¥  0.01ì„ ê³±í•˜ì—¬ ë¹¼ì¤Œ -> ì—…ë°ì´íŠ¸
   optimizer.step()
```



## logistic regression

0 or 1ì— ê°€ê¹Œìš´ ê°’ì„ ì°¾ëŠ” ë¶„ë¥˜ ëª¨ë¸

training dataë¥¼ ì˜ êµ¬ë¶„í•˜ëŠ” ê²½ê³„ì¸ ì´ˆë¡ìƒ‰ ì„ ì„ ì°¾ëŠ” ê²ƒ!

![image-20220826152149178](3_ì§€ë„í•™ìŠµ.assets/image-20220826152149178.png)

sigmoid í•¨ìˆ˜ ë„ì…

![image-20220826165740250](3_ì§€ë„í•™ìŠµ.assets/image-20220826165740250.png)

### hypothesis

![image-20220826165838122](3_ì§€ë„í•™ìŠµ.assets/image-20220826165838122.png)

`hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))`



### cost function

![image-20220826170030576](3_ì§€ë„í•™ìŠµ.assets/image-20220826170030576.png)

linear regressionì˜ ê²½ìš° MSEë¥¼ ì´ìš©í•˜ì—¬ lossë¥¼ êµ¬í•¨

logistic regressionì€ binary cross-entropyë¥¼ ì´ìš©í•˜ì—¬ loss êµ¬í•¨

`losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))`

`cost = losses.mean()`



### gradient decent

![image-20220826170221951](3_ì§€ë„í•™ìŠµ.assets/image-20220826170221951.png)

### ì‹¤ìŠµ

1. ë°ì´í„° ì´ˆê¸°í™” ë° parameter ì°¾ê¸°

```python
x_data = [[1, -4], [1, 2], [2, 3], [3, 1],[4, -2], [4, 3], [5, 3], [6, 2], [3, 8], [6, -2]]
y_data = [[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]]

# torch.Tensor()ì˜ ê¸°ë³¸ í…ì„œ íƒ€ì…: torch.FloatTensor
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

# ëª¨ë¸ ì´ˆê¸°í™”
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer ì„¤ì •
optimizer = optim.Adam([W, b], lr=0.01)

# ë°˜ë³µí•  íšŸìˆ˜ ì •í•˜ê¸°
nb_epochs = 5000

for epoch in range(nb_epochs + 1):

    # hypothesis ê³„ì‚°
    hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))

    # Cost ê³„ì‚°
    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()

    # optimizerì™€ costë¥¼ ì´ìš©í•´ ëª¨ë¸ íŒŒë¼ë©”í„° W,bë¥¼ ê°œì„ 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    # 100ë²ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))
        
# Epoch 5000/5000  Cost: 0.008265
# Costê°€ 0.01 ì´í•˜ë¡œ ì‘ì•„ì§ˆ ê²½ìš° í•™ìŠµì´ ì˜ ëœê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ

print(W)
# tensor([[4.2946],[1.6587]], requires_grad=True)
print(b)
# tensor([-18.1555], requires_grad=True)
```



2. ì‹œê°í™” í•˜ê¸°

```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

x_data = [[1, -4], [1, 2], [2, 3], [3, 1],[4, -2], [4, 3], [5, 3], [6, 2], [3, 8], [6, -2]]
y_data = [[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

x = x_train
y = y_train

# grad ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš° numpy arrayë¡œ ë³€í™˜ : detach().numpy()
W_x = W[0][0].detach().numpy()
W_y = W[1][0].detach().numpy()
b = b.detach().numpy()

for i in range(len(x_data)):
  # ê²°ê³¼ê°€ 0ì´ë¼ë©´
  if y_data[i][0]:
    plt.scatter(x_data[i][0], x_data[i][1], color='1', edgecolor="r")
  # ê²°ê³¼ê°€ 1ì´ë¼ë©´
  else:
    plt.scatter(x_data[i][0], x_data[i][1], color='0.75', edgecolor="b")

    
t = np.arange(0.,7.,0.001)
plt.plot(t, -(W_x/W_y)*t- b/W_y)
plt.show()
```

![image-20220826172723031](3_ì§€ë„í•™ìŠµ.assets/image-20220826172723031.png)