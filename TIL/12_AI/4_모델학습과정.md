# AI 모델 학습과정

구글 코랩 상단의 메뉴 -> 런타임 -> 런타임 유형 변경 -> 하드웨어 가속기: GPU or TPU



## 목표

Linear Layer만을 쌓은 Fully connected image classification model을 통해 모델이 학습 되는 과정 체험



* 필요한 라이브러리 import

  ```python
  import torch
  import torch.nn as nn
  import torchvision.datasets
  import torchvision.transforms as transforms
  from torch.autograd import Variable
  ```

* data 넣어주기

  ```python
  input_size = 784 # img_size = (28,28) ---> 28*28=784 in total
  hidden_size = 500 # hidden layer의 노드 개수
  num_classes = 10 #  [0,9]까지 class의 개수
  num_epochs = 10
  batch_size = 100
  lr = 1e-3 # learning rate로 backpropagation시 움직이는 스텝의 크기를 결정 
  ```

  * input_size
    * 28*28 = 784 pixel
  * hidden_size
    * hidden layer의 노드의 개수
    * hidden layer
      * input과 output 사이에 있는 layer
      * ![image-20220828224354568](4_모델학습과정.assets/image-20220828224354568.png)
      * the function applies weights to the inputs and directs them through an [activation function](https://deepai.org/machine-learning-glossary-and-terms/activation-function) as the output
  * num_classes
    * 0~9까지 class의 개수
    * 우리는 숫자를 판별할 것이기 때문에 숫자의 개수
  * num_epochs
    * 전체 트레이닝 셋이 신경망을 통과한 횟수
  * batch_size
    * 전체 트레이닝 데이터 셋을 여러 작은 그룹을 나누었을 때 batch size는 하나의 소그룹에 속하는 데이터 수 의미
    * 전체 트레이닝 셋을 작게 나누는 이유
      * 트레이닝 데이터를 통째로 신경망에 넣으면 비효율적 -> 학습 시간이 오래 걸림
  * lr
    * learning rate(학습률)
    * 너무 크면
      * parameter W가 발산해버림
    * 너무 작으면
      * 하나 최종 값을 찾을 때까지 너무 많은 반복을 해야함

* MNIST data를 다운로드 -> 저장

  ```python
  train_data = torchvision.datasets.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True)
  test_data = torchvision.datasets.MNIST(root = './data', train = False, transform = transforms.ToTensor())
  ```

  